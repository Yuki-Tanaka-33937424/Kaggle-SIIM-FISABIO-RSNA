# Kaggle-SIIM-FISABIO-RSNA<br>
KaggleのSIIM-FISABIO-RSNA COVID-19 Detectionコンペのリポジトリです。<br>

## 方針
- ファイルはgitで管理する。ただし頑張り過ぎない。<br>
- Notebookやスクリプトはsiim_○○○_ver□□_hoge.ipynb(nb)の形で統一する。<br>

## Overview(Deepl)<br>
### **Description**<br>
COVID-19は、インフルエンザの5倍の致死率で、重大な罹患率と死亡率を引き起こします。他の肺炎と同様に、COVID-19に肺感染すると、肺に炎症や水が溜まります。COVID-19は、胸部X線写真では他のウイルス性や細菌性の肺炎と非常によく似ているため、診断が困難です。COVID-19を検出して位置を特定するコンピュータビジョンモデルがあれば、医師が自信を持って迅速に診断を下すことができます。その結果、患者はウイルスの最も深刻な影響を受ける前に、適切な治療を受けることができます。<br>

現在、COVID-19の診断は、ポリメラーゼ連鎖反応でウイルスの遺伝物質を検出する方法と、胸部X線写真で行う方法があります。しかし、分子検査の結果が出るまでには、数時間から時には数日かかることもあります。これに対し、胸部X線写真は数分で撮影できます。放射線科医がCOVID-19を他のタイプの感染症と区別するためのガイドラインは存在するが、その評価は様々である。また、放射線科医以外の医師は、視覚的なバウンディングボックスなどを用いて、疾患の位置をより明確にすることでサポートすることができます」と述べています。<br>

Society for Imaging Informatics in Medicine（SIIM）は、教育、研究、革新を通じて医療画像情報学を推進することを使命とする、この分野における主要なヘルスケア団体です。SIIMは、バレンシア地方健康・生物医学研究促進財団（FISABIO）、バレンシア地方医用画像データバンク（BIMCV）、北米放射線学会（RSNA）と協力して、このコンペティションを開催します。<br>

このコンテストでは、胸部X線写真上でCOVID-19の異常を識別し、位置を特定します。特に、肺炎の陰性、COVID-19の典型、不確定、非典型のいずれかに分類します。あなたとあなたのモデルは、複数の放射線科医が撮影した画像データと注釈を用いて作業を行います。<br>

成功すれば、放射線科医が何百万人ものCOVID-19患者をより自信を持って迅速に診断できるようになります。また、医師が病気の範囲を確認し、治療に関する意思決定に役立てることができます。重症度によっては、入院や集中治療室への入室、人工呼吸などの支持療法が必要になる場合もあります。より良い診断の結果、より多くの患者さんが迅速に最適な治療を受けることができ、ウイルスの最も深刻な影響を軽減することができます。<br>

### **Evaluation**<br>
この課題では、標準的なPASCAL VOC 2010の平均平均精度（mAP）をIoU > 0.5で使用しています。なお、リンク先の資料にはVOC 2012が記載されていますが、VOC 2010とは細かい点で異なります（例：VOC 2010には「難しい」クラスの概念がありません）。P/R曲線やAPの計算方法は変わりません。<br>

本大会では、スタディ（複数画像）レベルと画像レベルの両方で予測を行っています。<br>

**Study-level labels**<br>
テストセットのスタディには、複数のラベルが含まれている場合があります。それらは以下の通りです。<br>

"negative"、"typical"、"indeterminate"、"atypical"<br>

詳細は「データ」のページをご覧ください。<br>

テストセットの各研究について、上記のラベルのうち少なくとも1つを予測する必要があります。与えられたラベルの予測のフォーマットは、上記リストからのクラスID、信頼度スコア、0 0 1 1は1ピクセルのバウンディングボックスとなります。<br>

**Image-level labels**<br>
テストセットの画像には、複数のオブジェクトが含まれている場合があります。与えられたテスト画像の各オブジェクトに対して，クラスIDとして "opacity"，信頼度スコア，バウンディングボックスをxmin ymin xmax ymaxの形式で予測する必要があります．ここで，noneは "No finding "のクラスID，1.0は信頼度，0 0 1 1は1ピクセルのバウンディングボックスを表しています．<br>

**Submission File**<br>
投稿ファイルは，ヘッダを含み，以下の形式である．<br>

Id,PredictionString
2b95d54e4be65_study,negative 1 0 0 1 1
2b95d54e4be66_study,typical 1 0 0 1 1
2b95d54e4be67_study,indeterminate 1 0 0 1 1 atypical 1 0 0 1 1
2B95D54E4BE68_画像,なし 1 0 0 1 1
2b95d54e4be69_image,opacity 0.5 100 100 200 200 opacity 0.7 10 10 20 20<br>

## Data(Deepl)<br>
この競技では、胸部X線写真上のCOVID-19の異常を識別し、局所化しています。これは、物体検出と分類の問題です。<br>

各テスト画像について、すべての所見のバウンディングボックスとクラスを予測することになります。所見がないと予測した場合は、「none 1 0 0 1 1」という予測を作成します（「none」は所見がないことを示すクラスIDで、これは信頼度1.0の1ピクセルのバウンディングボックスを提供します）。<br>

さらに、各試験研究について、以下のラベル内で判定を行う必要がある。<br>

肺炎陰性」「典型的な外観」「不確定な外観」「非典型的な外観」。<br>

上記のラベルの1つを予測するには、上記の「none」クラスと同様の予測文字列を作成します：例： atypical 1 0 0 1 1<br>

予測値のフォーマットについての詳細は、「評価」のページを参照してください。<br>

画像はDICOM形式なので、視覚化や分類に役立つ追加データが含まれています。<br>

データセット情報<br>
訓練データセットは、DICOM形式の6,334枚の胸部スキャンで構成されており、患者のプライバシーを保護するために非識別化されています。すべての画像は，経験豊富な放射線科医のパネルによって，不透明性の有無と全体的な外観についてラベル付けされています．<br>

すべての画像はstudy/series/imageという形式のパスに格納されていることに注意してください。ここでのstudy IDはstudyレベルの予測に直接関係し、image IDはimageレベルの予測に使われるIDです。<br>

隠れたテストデータセットは，学習データセットとほぼ同じ規模のものです．<br>

#### Files<br>
train_study_level.csv - 学習データレベルのメタデータで，各学習データごとに1行あり，正しいラベルも含まれています．<br>
train_image_level.csv - 訓練画像レベルのメタデータで，各画像ごとに1行，正しいラベルと辞書形式のバウンディングボックスが含まれています．test, trainともに，複数のバウンディングボックスを持つ画像があります．<br>
sample_submission.csv - 全ての画像レベルと研究レベルのIDを含むサンプル投稿ファイル。<br>

#### Columns<br>
**train_study_level.csv**<br>
id - 一意の研究識別子<br>
Negative for Pneumonia - 肺炎が陰性の場合は1、それ以外は0。<br>
典型的な外観 - このような外観の研究であれば1、それ以外は0<br>
不確定な外観 - 研究がこのような外観である場合は1、そうでない場合は0<br>
非典型的な外観 - このような外観の場合は1、それ以外は0<br>

**train_image_level.csv**<br>
id - 一意な画像識別子<br>
boxes - 簡単に読める辞書形式のバウンディングボックス<br>
label - 与えられたバウンディングボックスに対する正しい予測ラベル<br>

#### Citation<br>
この課題で使用されているBIMCV-COVID19データは、The Foundation for the Promotion of Health and Biomedical Research of Valencia Region (FISABIO)とThe Regional Ministry of Innovation, Universities, Science and Digital Society (Generalitat Valenciana)の協力のもと、Medical Imaging Databank of the Valencia Region (BIMCV)によって公開されたものですが、画像は異なるアノテーションタイプを用いて完全に再アノテーションされています。このデータの利用者は、BIMCV-COVID19 Dataset research Use Agreementに従わなければなりません。論文参照 BIMCV COVID-19+：COVID-19患者のRXおよびCT画像の大規模なアノテーション付きデータセット<br>

この課題で使用されたMIDRC-RICORDデータは、元々The Cancer Imaging Archiveによって公開されたものです。この課題のために、画像は異なるアノテーション・スキーマを用いて再アノテーションされました。このデータを使用する際には、TCIA Data Usage Policyおよび公開されているCreative Commons Attribution-NonCommercial 4.0 International Licenseを遵守する必要があります。アトリビューションには、TCIAの引用情報ページ（ページ下部）に記載されている引用文献への参照を含める必要があります。論文の参照先 The RSNA International COVID-19 Open Radiology Database (RICORD)<br>

## Log<br>

### 20210621<br>
- join !!<br>
- 色々コンペに関して不明な点や物体検出に関してわからない点が多かったので初日はEDAと調査に費やした。色々わかったことを書いていく。<br>
  - タスクについて。今回は各画像に対して画像分類と物体検出を両方行っていく。study_levelに対しては画像分類を、image_levelに対しては物体検出を行う。study_levelは6054枚、image_levelは6054枚あり、前者は後者のStudyInstanceUIDに対応してるらしい。だから、画像分類に関しては全部の画像は必ずしも使わなくてかもしれない。<br>
  - 評価指標について。mAPの説明に関しては[このQiita記事](https://qiita.com/cv_carnavi/items/08e11426e2fac8433fedk)がわかりやすかった。その中のconfidenceなどに対する説明は[このQiita記事](https://qiita.com/cv_carnavi/items/68dcda71e90321574a2b)がわかりやすかった。ついでにYoloも色々学べた。<br>
  - study_levelのラベルについて。trainデータのラベルは全て1種類のみだが、Evaluationのページはなぜか2種類予測しているところがあり、文章でも"Studies in the test set may contain more than one label."とある。[このDiscussion](https://www.kaggle.com/c/siim-covid19-detection/discussion/240250#1315782)におけるホストの回答では、各annotatorは1つのラベルのみを付与しているが、今回の提出format上、2つ以上のラベルを予測することを拒否できないからと回答している。今のところは基本的に一種類で予測してしまっていいような気がする。<br>
  - モデルについて。大体はtfを使っていてコードがよくわからなかった。そのまま動かしてもいいが、いまいちやった感がないのでは勉強にならないので、[このNotebook](https://www.kaggle.com/piantic/train-siim-covid-19-detection-fasterrcnn/data)を参考にしてしっかり自分のコードに落とし込んでから色々なモデルを試していこうと思う。<br>
- 001(EDA)<br>
  - ver1<br>
    - ここでわかったことは割と上で書いてしまったが、そうでないところを書いていく。<br>
    - image_levelに関して、StudyInstanceUIDで被りがあるため、そこはgroup化してvalidationをしないとリークになるだろう。ただ、StudyInstanceUIDが同じ画像群はほとんど画像が同じである上にbboxが付与されているものが一つしかないため、結局そのうちの一つだけを使うことになりそう。そうなると別にgroup化する必要はなくなるか。[このDiscussion](https://www.kaggle.com/c/siim-covid19-detection/discussion/246597)におけるホストの回答からの情報。<br>
    - study_levelに関して、クラスの偏りはあるがそう大きくはないような気がする。いったん気にせず学習させて、予測精度に偏りが見られたらFocal Lossなどで対応していく感じでいいかな？<br>

### 20210625<br>
この三日間、ひたすらDetectionのモデルを動かすのに苦戦していた。<br>
- AdamのWeight Decayは若干おかしいため、AdamでWeight Decayを使うぐらいならAdamWを使うのがいいらしい。自分はいつもAdamでWeight Decayを使ってきたので、これを機にAdamWに乗り換えてみる。<br>
- 002(EffDetd0)<br>
  - 本当は上述の通りにFasterRCNNを使おうとしていたが、どうしても推論の時にモデルが出力をしてくれず、結局断念した。また、参考にしようとしていたNotebookもおかしかったのでどうすることもできなかった。無念。<br>
  - 実は、参考にしていたNotebookは、[小麦コンペ](https://www.kaggle.com/c/global-wheat-detection)の[EfficientDetのNotebook](https://www.kaggle.com/shonenkov/training-efficientdet)をほぼそのまま使っていたようなので、こちらを参考にすることにした。upvote数が半端じゃないぐらい高い上に推論のコードもしっかり用意されているので、とても良さそう。小麦コンペにはDetectionに関して参考になるNotebookが非常にたくさんあるため、他のモデルについても参考にしていきたい。<br>
  - そのままEfficientDetを使うことにした。小麦コンペの上位解法は大体EfficientDetで構成されているので信用できそう。ただ、今後の勉強のためにどこかのタイミングでFasterRCNNはフルスクラッチで実装できる必要がある。<br>
  - ver1<br>
    - とりあえずD0にした。最初は色々様子を見たいので軽い方がいい。<br>

### 2020627<br>
- 002<br>
  - ver2<br>
    - modelを呼び出そうとしたらload_state_dictでエラーが起こってしまったのでそれについて調べた。今回はなぜか、state_dictの中身がmodel.mode.hogeの形になっていて、読み込む側はmodel.hogeの形になっているためにミスマッチが起こっている。いつもはstate_dict側もmodel.hogeの形になっているため問題がなかった。恐らく今回のDetBenchTrainあたりが何か裏でやっているのかもしれない。それ以外は状況が同じなので、それぐらいしか考えられない。今回は目を瞑ってsaveするときにmodel.model.state_dict()として解決することにする。<br>
    - それに伴って、学習をやり直すことにした。<br>
    - CVを出せるようにコードを追加する。CVを出すにあたって、[このNotebook](https://www.kaggle.com/pestipeti/competition-metric-details-script/comments)を参考にした。ただ、小麦コンペのprecisionは分母にfalse segativeも入っているがSIIMでは含まれていないはずだから、そこは抜かないといけない。<br>
    - やっぱり、小麦コンペのものをそのまま使っても正しくスコアが出せないと見た(多分presicion-recallの曲線の積分ができてない？)ので、[titoさんのNotebook](https://www.kaggle.com/its7171/map-understanding-with-code-and-its-tips)を参考にすることにした。ここで疑問に思うのが、grand-truthがない場合のAPの数値について。一つも予測しなければnoneとして予測してpresicionもrecallも1の状況ができるからAPは1でよくて、一つでも予測してしまった時点で0になると言う認識で良さそう。<br>
    - version関係で死ぬほど苦戦して、結局[titoさんのNFLのNotebook](https://www.kaggle.com/its7171/2class-object-detection-inference)を参考にしてなんとか動かせた。闇が深い。<br>
    - 動かせたが、lossの落ち方が若干悪くなってしまった。どこが原因かよくわからないので探る必要がある。また、mAPが0になってしまった。恐らくnoneの予測の仕方が間違っているので、考え直さなければいけない。<br>
  - ver3<br>
    - noneの予測の仕方について。思いついているのは次の通り。<br>
      - 全部opacityで予測して、面積が異常に小さいやつは全てnone扱いにする。<br>
      - 普通に2クラスで予測させる。閾値に2クラス分類モデルを使う。<br>
      - opacityのみについて学習し、予測の際には2クラス分類でopacityが存在すると予測されたものだけを対象にする。予測がなかったものに関してはnoneを入れる。<br>
    - 筋が良さそう(2値分類モデルとの相性が良さそう)なのは2番目と3番目だと思う。2値分類がそんなに強くないなら2番目、強いなら3番目にしていいと思うが、それは実験をしてみないとわからなさそう。<br>
    - 2クラスでbboxを予測するコードをとりあえず書いたところで終わった。
### 20210629<br>
- 002<br>
  - ver3<br>
    - 色々デバッグに苦しんだ。リストではさまざまな型のデータを同時に入れることができるが、ndarrayではそれができないためにnp.stack(list)とした時点でデータ型が全て統一されてしまう仕様に気づかず、モデルの出力がいつの間にか全て文字列になってしまっているのがどうしてかわからなかった。ちなみに、mAPを出す関数内では改めて型を変換しているのでそのまま突っ込んで大丈夫。<br>
    - 中途半端にクラス名を0・1にしたりnone・opacityにしたりしてしまったが、全部0・1に統一して、subする時だけnone・opacityを書いた方がいい。<br>
    - デフォルトだとmAPは0で、後処理を頑張って初めてnoneクラスのmAPが0.2程度になる。opacityクラスは、IOU_thresholdを0.01にしてもmAPが0なので、全く当て切れていない。モデルを大きくしたり画像サイズを大きくしたりして少し粘ってみて、そのあとは2クラス分類モデルと組み合わせる。<br>
- 003(2class_train_Eff)<br>
  - ver1~4<br>
    - stei_002_ver7を引っ張ってきて、各画像がopacityなのかnoneなのか学習させるモデルを作った。モデルはEffb0。<br>
    - | ver | ver1との差分 | CV(AUC) |
      | :---: | :---: | :--- |
      | 1 | - | 0.87024 |
      | 2 | バッチサイズを64->128にした | 0.86923 |
      | 3 | 画像サイズを256->384にした | 0.87431 |
      | 4 | 画像サイズを256->512にした | 0.87152 |<br>
    - bboxがあるかどうかの2値分類はやはりそれなりに難しいみたい。大幅に改善するには外部データを持ってくるなどしなければいけないと思われるので、とりあえずこの精度でできることをしたい。<br>

### 20210630<br>
- 002<br>
  - ver4<br>
    - 発展DeepLearningと[arutemaさんのNotebook](https://www.kaggle.com/kyoshioka47/pytorch-centernet-inference-with-tta)を参考にしてnmsを実装した。多少はbboxが減っていたようなので、いい方向に働くはず。<br>
  - ver5<br>
    - モデルをD1に変更した。<br>
  - ver6<br>
    - モデルをD2に変更した。nmsのところでエラーが出てしまったので、恐らくconfの値で全て切られてしまったと思われる。modelごとにconfの値が大きく違うのは結構気になる。<br>
- 003<br>
  - ver5, 6<br>
    - ver3から、モデルをEffv2に変えてみる。<br>
    - | ver | model | CV |
      | :---: | :---: | :---: |
      | 5 | tf_efficientnetv2_s | 0.88113 |
      | 6 | tf_efficientnetv2_s_in21ft1k | 0.88085 |<br>
    - CVは伸びたが、モデルがかなり重たい。だったら普通にb0でいいと感じた。<br>

### 20210701<br>
- 002<br>
  - ver7<br>
    - confのthresholdを0.01にして再挑戦する。<br>
  - ver8<br>
    - EffDetD3。ここでOOMが起きたのでバッチサイズを64から32に下げた。<br>
  - ver9~ver12<br>
    - モデルを大きくし続けた。実験結果は以下の通り。<br>
    - | ver | model | batch_size | loss |
      | :---: | :---: | :---: | :---: |
      | 04 | D0 | 64 | 8.6876 |
      | 05 | D1 | 64 | 5.8122 |
      | 07 | D2 | 64 | 4.8656 |
      | 08 | D3 | 32 | 1.3821 |
      | 09 | D4 | 32 | 1.2380 |
      | 10 | D5 | 24 | 1.1016 |
      | 11 | D6 | 16 | 0.9420 |
      | 12 | D7 | 16 | 0.9019 |<br>
    - D3から先が一気にLossが落ちているため、当面の間はD3で実験をしていく。<br>
  -
- 004(4class_train_EffNet)<br>
  - ver1~3<br>
    - study_classの予測のためのモデル。<br>
    - てっきり確率が一番高いクラスのconfを1にして提出するのかと思いきや、公開Notebookを見ていると確率をそのままconfにして、4クラス全てを予測して提出していた。どっちがCVが高くなるか確認する必要がある。<br>
    - 公開Notebookをみる限りではimage_classよりもstudy_classの方がスコアが高いので、こっちをしっかり詰める必要がある。<br>
    - RANZCRのときに公開されていた重みがそのまま今回も使えるかもしれないので、それも考えてEffB3で始めようと思う。<br>
    - 画像サイズを色々変えて実験した。<br>
    - | ver | size | Score(AUC) |
      | :---: | :---: | :---: |
      | 01 | 256 | 0.7706 |
      | 02 | 384 | 0.7760 |
      | 03 | 512 | 0.7811 |<br>
    - そこまで変わらなかった。とりあえずCVもだす。<br>
### 20210702<br>
- 002<br>
  - ver13, 14<br>
    - 画像サイズを変えて実験した。<br>
    - | ver | size | loss |
      | :---: | :---: | :---: |
      | 08 | 256 | 1.3821 |
      | 13 | 384 | 1.1065 |
      | 14 | 512 | 1.0010 |<br>
    - モデルのサイズからしても画像のサイズからしても、まだまだLossが落ちそう。<br>
  - ver15, ver16<br>
    - 256の画像を使って拡大してしまっていたため、512の画像を使ってやり直す。<br>
    - また、nmsは1画像ごとにconfが高いbboxをtop_k個取ってきて、その中で重複しているbboxを消すものであって、EffDetは１画像に対して100個の予測をしするため、top_kは200ではなく100で十分であることがわかった。10程度に削ってもいいが、mAPをあげるにはとりあえず当てきれないといけないので予測は積極的に残すべき(下位に正解があるのにそれを切ったら損するだけ)。<br>
    - | ver | size | loss |
      | :---: | :---: | :---: |
      | 08 | 256 | 1.3821 |
      | 15 | 384 | 1.1301 |
      | 16 | 512 | 0.9952 |<br>
  - ver17<br>
    - opacityクラスだけを学習するように修正した。<br>
    - loss: 0.9934。ほぼ変わらないので実装はあっていると思う。<br>
- 005(detection_caliculate_CV)<br>
  - ver1, 2<br>
    - それぞれ002-ver13, ver14のmAPを計算した。<br>
    - | ver | 002-ver | mAP |
      | :---: | :---: | :---: |
      | 01 | 13 | 0.1596 |
      | 02 | 14 | 0.1793 |<br>
    - ただし、これらは全てnoneクラスに対するmAPである。本来、noneクラスは2クラス分類モデルで当てるため、detectionモデルではopacityをしっかり当てきれないといけない。まだまだ改善が必要...<br>
    - ver3<br>
      - 003-ver6のモデルの予測を使ってnoneを入れた。<br>
      - mAPが0.4303まで上がった。明らかによくなったので、やはりdetectionはopacityだけを予測して、noneは2クラス分類で予測する方が筋がいいみたい。<br>
      - ちなみに、detectionの予測も利用してnoneの精度をあげようとしたがほぼ上がらなかった。<br>
    - ver4<br>
      - 各bboxの正誤判定は画像ごとに行われるが、それぞれのtrue or falseと一緒にconfが一緒に保存されて、最後にconfをthresholdとして動かして積分してmAPを出すから、予測にしっかりモデルの自信を反映させることが大事であることがわかった。今回の2クラス分類のモデルはopacityがある確率を出すため、noneのconfには1 - probの値を入れればいい。そうすると**mAPは0.4303から0.4916まで上がった**。今後はこれでいく。<br>
### 20210703<br>
- 003<br>
  - ver7<br>
    - モデルをEffcientNetB3に変える。<br>
    - Score(AUC): 0.89131。EfficientNetv2_sより軽いし性能もよかった。v2はなんだったんだろう。<br>
  - ver8<br>
    - mAPを算出できるようにした。<br>
    - mAPが0.3620で、005-ver4よりかなり低かった。なぜだろう...<br>
  - ver9<br>
    - ver8で戸惑ったmAPについては、opecityの0が入った平均になっていた。なので、クラスnoneに関してはその倍の値のAPが出ていた。ついでにモデルの保存に使うscoreもmAPにするなど色々整えた。<br>
    - ということは、005のCVも、noneは確率0.5以上ではなく全部入れてしまうようにすればもっと値が高くなるはず。<br>
    - mAP: 0.7240<br>
- 004<br>
  - ver4<br>
    - 各クラスのmAPを算出できるようにした。それ以外はver3と同じ。<br>
    - mAP: 0.5224。こっちをあげる方が優先かもしれない。<br>
- 005<br>
  - ver5<br>
    - 003-ver9で気づいた通りにmAPを算出したら0.7153まで上がった。AUCが0.8を超えていることからも、妥当な値だと思われる。<br>
    - 公開Notebookでは、imageクラスのmAPは大体0.16であるということは、opacityとnoneのmAPの平均はその3倍の0.48ぐらい。現段階で0.7153*(1/2)=0.3577程度出ているため、思った以上にopacityクラスのmAPは低そう。noneクラスのmAPが今のままだとしても、opacityのmAPが0.25程度出てしまえば追いつける。(0.000021しか出てないけど...)<br>
### 20210704<br>
- 002<br>
  - ver18<br>
    - ローカルで動かせるようにデータセットなどを整えた。また、mAPをちゃんと出力・記録できるようにして、annotationsとpredictionsはDataFrameにしてからpickleで保存するようにした。<br>
    - bboxの変換を最初にして、DataSet内では変換しないようにして1epochあたり1分程度高速化した。<br>
    - バッチサイズを8から16に上げた。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 1.0184 | 1.2654 | 0 |<br>
    - バッチサイズを倍にしたので学習しきれなくなってる。公平な比較のために学習率は同じ比率であげるべきだった。ここまで学習率はいじらずにきたので、もしかしたらまだまだ学習できるかもしれない。物体検出のモデルの勘所が掴めないが、他のどのモデルを見ても30~50epochぐらいは回しているので、もっと気長に待つべきなのかもしれない。<br>
  - ver19<br>
    - とりあえず学習率を上げていきつつ様子を見る。本来ここでハイパラを探索するのは得策ではないが、学習率は実験を回すスピードにもろに影響するので上げられる分は上げていいと思う。<br>
    - | ver | lr | train loss | valid loss | CV |
      | :---: | :---: | :---: | :---: | :---: |
      | 18 | 1e-4 | 1.0184 | 1.2654 | 0 |
      | 19 | 2e-4 | 0.8776 | 1.0328 | 0 |
      | 20 | 3e-4 | 0.8387 | 1.0053 | 0 |<br>
    - バッチサイズに合わせて学習率はあげるべきだった。また、今回は最小値も変えてしまったがそれは変えないべきだったかもしれない。しょうもない実験だけどいまいち見えないのでもう少し続けようと思う。<br>
  - ver21<br>
    - ver18からepochを30に増やしてみた。<br>
    - | ver | epoch | train loss | valid loss | CV |
      | :---: | :---: | :---: | :---: | :---: |
      | 18 | 10 | 1.0184 | 1.2654 | 0 |
      | 21 | 30 | 0.7916 | 1.0428 | 0 | <br>
    - epochが足りていないわけではなかった。ひとまず10のままで固定する。<br>
### 20210705<br>
- そういえばwandbを導入しようとしていたのを忘れてた。いちいちここに表形式でlossを書くぐらいなら使った方がいい気がする。<br>
- 002<br>
  - ver22<br>
    - ver20から最小値だけ元に戻す。恐らくこれは画像認識の方でも同じようにした方がいい。学習の初動はパラメータが進む方向は定まりやすいはずで、その時は学習率は大きくすべきだが後半はそうではないと考えられる。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.8391 | 1.0053 | 0 | <br>
    - ほぼスコアは同じだが後半は安定していたのでこっちにする。<br>
  - ver23<br>
    - transformで、validationの時だけNormalizeが入ってたので抜く。今回はDatasetの中で簡易的にNormalizeをしているため、transformで入れる必要はない。もしかしたらこれが原因で精度がでてなかったのかも。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.8390 | 0.9337 | 0 | <br>
    - 無事にlossは落ちたが、それでもmAPは0のままなので、もう少しlossを下げに行った後に根本的な誤りを見つけに行った方がいいと感じる。<br>
  - ver24<br>
    - augmentationを少し強めた。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.8128 | 0.9529 | 0 | <br>
  - ver25<br>
    - augmentationをさらに強めた。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.7952 | 1.0136 | 0 | <br>
    - augmentationを強くしているのにどんどん過学習が強くなってしまっている。全然わからん...とりあえずver23に戻して実験をする。<br>
  - ver26, ver27<br>
    - モデルを大きくした。<br>
    - | ver | model | train loss | valid loss | CV |
      | :---: | :---: | :---: | :---: | :---: |
      | 23 | D3 | 0.8390 | 0.9337 | 0 |
      | 26 | D4 | 0.7942 | 0.9196 | 0 |
      | 27 | D5 | 0.7440 | 0.8862 | 0 |<br>
    -  lossはどんどん改善しているが依然としてCVはゼロのまま。これはどう考えてもモデルの性能じゃなくて出力の途中でバグを埋め込んでるだろ...<br>
  - ver28<br>
    - titoさんNotebookをよくみたら、モデルの元々の出力は[xmin, ymin, xmax, ymax]ではなく[xmin, ymin, width, height]らしく、それを後から修正していた。恐らくこれを見落としていたんだと思う。<br>
    -
- 003<br>
  - ver10<br>
    - Kaggle Notebookで動かすために、バッチサイズを16に落とした。<br>
    - また、annotationsやpredictionsを保存しておくようにした。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.1999 | 0.3273 | 0.7188 | <br>
    - 案の定過学習気味になってしまった。めんどくさがらずに学習率も合わせて調整すべきだった。ただ、画像サイズが384に落ちたことによって多少はmAPも落ちるはずなので、妥当な数値ではある。<br>
  - ver11<br>
    - バッチサイズを16に落としたのに合わせて、学習率も半分に落とした。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3122 | 0.3280 | 0.6990 | <br>
    - なぜかうまくいかなかったのでver10に戻す。<br>
  - ver12<br>
    - Normalizeを全くしていないことが発覚したので修正した。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.2019 | 0.3361 | 0.7107 | <br>
    - 多少mAPが落ちているが、多分誤差の範囲内だしこっちの実装が正しいのでこのままいく。<br>
  - ver13<br>
    - augmentationを少し強めた。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.2779 | 0.3254 | 0.7239 | <br>
  - ver14<br>
    - さらにaugmentationを強めた。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3057 | 0.3183 | 0.7179 | <br>
    - 多少CVが落ちているが、lossはこちらが低く、これからモデルを大きくするので一旦これでいこうと思う。<br>
  - ver15<br>
    - 実はRANZCRの時のpretrainedのモデルはB5だったので、モデルのサイズを上げていく。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3411 | 0.3354 | 0.7081 | <br>
    - 全然安定しないので少しepochを伸ばす必要があるかもしれない。また、CVは基本的に高い方を採用していくべきなのでver13に戻す。あまり時間が取れずに色々並列に回しているので至らないところが増えてきてるな...<br>
- 004<br>
  - ver5<br>
    - Kaggle Notebookで動かすために、バッチサイズを16に落とし、画像サイズも上げっぱなしだったので384に落とした。<br>
    - また、annotationやpredictionsを保存しておくようにした。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3174 | 0.3919 | 0.5271 | <br>
  - ver6<br>
    - Normalizeされていないバグを修正した。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3520 | 0.3957 | 0.5218 | <br>
  - ver7<br>
    - augmentationを少し強めた。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3633 | 0.3810 | 0.5232 | <br>
  - ver8<br>
    - augmentationをさらに強めた。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3692 | 0.3839 | 0.5174 | <br>
    - 003と同様に悪化してしまった。lossとCVの挙動が若干不安定なので、一度epochを増やしてみる。<br>
### 20210706<br>
- 003<br>
  - ver16<br>
    - ver13に戻して、epochを10に増やした。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3001 | 0.3239 | 0.7227 | <br>
    - epoch5でこれを出した後はゴリゴリに過学習してしまった。epochは6のままにしておく。<br>
- 004<br>
  - ver9<br>
    - ver7に戻してepochを10に伸ばした。<br>
    - | train loss | valid loss | CV |
      | :---: | :---: | :---: |
      | 0.3671 | 0.3834 | 0.5320 | <br>
    - こちらもepoch5でこれを出した後に過学習してしまったので戻す。スコアが伸びたのはlossをみると偶然だろうと考えられる。<br>
- 005<br>
  - ver5<br>
    - 002-ver28のpredictionsを詳しくみた結果、一部の画像に対する出力の座標が0~512に収まっていないことがわかった。元のtitoさんのNotebookでは単純にclipしているので、モデルの組み方や出力のさせ方が悪いわけではなさそう。clipしたらmAPが若干上がった(0.000455->0.000502)。
    - また、出力が直線x=yで反転していたことがわかった。直してmAPを計算すると、0.000502から0.249239(noneも合わせた平均は0.485954)まで上がった。ようやくまともな数値まで上がった。もっと早く見ればよかった。コンペの度に公開しているのでいい加減反省を生かさないといけない。<br>
    - noneに対するconfが高い時にopacityの予想を切ったらわずかに上がって0.486999になった。<br>
    - また、画像1枚ずつに対して個別にmAPを計算してから平均を取ると、opacityとnoneの平均で0.61まで上がった。上がるのは当然なのだが、LBがどっちで出しているのか確信が持てないのでこればかりはサブをしないとわからなさそう。十中八九前者だろうけど。<br>
### 20210708<br>
- 002<br>
  - ver29<br>
    - 005-ver6にしたがってモデルの出力のさせ方を変えた。<br>
    - | train loss | valid loss | mAP |
      | :---: | :---: | :---: |
      | 0.7426 | 0.8887 | 0.254139 | <br>
    - ようやくちゃんとしたmAPが出せた。<br>
- 003<br>
  - ver16, ver17, ver19<br>
    - モデルを大きくした。<br>
    - | ver | model | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: | :---: | :---: |
      | 13 | B3 | 0.2779 | 0.3254 | 0.7239 |
      | 17 | B4 | 0.2779 | 0.3258 | 0.7105 |
      | 18 | B5 | 0.2970 | 0.3272 | 0.7127 |<br>
    - lossとCVが全然噛み合ってないのがすごい気になる。ずっと過学習気味で実験結果が比較しづらいので少し学習率を落とすかweight decayを強めるかをしてみる。<br>
- 004<br>
  - ver10, ver11<br>
    - モデルを大きくした。<br>
    - | ver | model | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: | :---: | :---: |
      | 7 | B3 | 0.3633 | 0.3810 | 0.5232 |
      | 10 | B4 | 0.3506 | 0.3896 | 0.5253 |
      | 11 | B5 | 0.3175 | 0.3793 | 0.5470 |<br>
    - モデルを大きくするとmAPも大きくなっているので、まだスコアが伸びる余地が残っているかもしれない。ただ、lossの下がり方はそこまで大きくないので偶然だった可能性もある。<br>

### 20210713<br>
- ここ数日の間ずっと続けてはいたのだが、まとまった時間が取れずにKaggle日記の更新も止まってしまっていた。こういうことはなくしていきたい。<br>
- 002<br>
  - ver30<br>
    - titoさんのNotebookをみたら、datasetのところでxy反転を行っていた。一応ちゃんとEfficientDetのリポジトリを見に行って入力がどうなっているのか確認したい。<br>
    - datasetで反転させたのに、make_predictionsの中で反転を消すのを忘れていたので、二重に反転させてしまった。<br>
- 003<br>
  - ver18~<br>
    - 過学習を防ぐためにWeight decayを変えた。このパラメータに関してはあんなmり触ることがなかったので、この際に大まかな挙動を掴みたい。<br>
    - | ver | weight  decay | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: | :---: | :---: |
      | 18 | 1e-6 | 0.2970 | 0.3272 | 0.7127 |
      | 19 | 1e-5 | 0.2960 | 0.3272 | 0.7127 |
      | 20 | 1e-4 | 0.2960 | 0.3272 | 0.7127 |
      | 21 | 1e-3 | 0.2919 | 0.3307 | 0.7164 |
      | 22 | 1e-2 | 0.2950 | 0.3254 | 0.7170 |
      | 23 | 1e-1 | 0.2251 | 0.3371 | 0.7178 |
      | 24 | 1e-0 | 0.3014 | 0.3215 | 0.7311 |
      | 25 | 1e+1 | 0.4146 | 0.4315 | 0.5942 |<br>
    - 表を見ると、1e-4あたりまではほとんど変わらず、1e-3から少しずつ変わり始めて1で一気に効果が表れて10でunder fittingになった。この辺はデータの正規化の仕方やモデルのサイズなどにもよるところではあると思うが、weight decayをしっかり効かせたいなら1e-3あたりにはしておかないといけないっぽい。<br>
- 004<br>
  - ver12<br>
    - RANZCRで使われていたpretrainedを使ってみた。<br>
      - | train loss | valid loss | mAP |
        | :---: | :---: | :---: |
        | 0.3624 | 0.3747 | 0.5434 | <br>
      - 普通に悪化した。<br>
- 006(infer_image)<br>
  - ver29-ver24-1<br>
    - imageクラスの推論を行うNotebook。今回はcode competitionなので推論のNotebookはgit管理しない。どうせ手元で回すことはないから。<br>
    - 最初のver29は002のversionで、次のver24は003のversionに対応して、最後の1は、002と003のverが被ったときの連番。<br>
    - | CV | mAP |
      | :---: | :---: |
      | 0.1647 | 0.160 |<br>
    - studyクラスの予測値を全てデフォルトの'negative 1 0 0 1 1'のままにしてしまったので、negativeクラスのスコアが入ってしまっている。全て空白にしないといけない。<br>
  - ver29-ver24-2<br>
    - studyクラスのPredictionStringは''にした。<br>
    - | CV | mAP |
      | :---: | :---: |
      | 0.1647 | 0.109 |<br>
    - 元画像のサイズで補正するときに縦横反対にしていた。実質noneクラスだけのスコアになってしまっている。<br>
  - ver29-ver24-3<br>
    - 元画像のサイズでの補正の仕方を修正した。<br>
    - | CV | mAP |
      | :---: | :---: |
      | 0.1647 | 0.180 |<br>
    - 若干上振れしているが大きなミスはないと考えられる。<br>
- 007(infer_study)<br>
  - 004-ver11<br>
    - studyクラスの推論を行うNotebook。006と同じくgitで管理することはない。<br>
    - studyクラスの画像は一部被りがあることを忘れていたために何度もエラーを出してしまった。<br>
    - | CV | LB |
      | :---: | :---: |
      | 0.5470 | 0.365 |<br>
### 20210714<br>
- 002<br>
  - ver31<br>
    - make_predictionsの中の反転を消した。<br>
    - | train loss | valid loss | mAP |
      | :---: | :---: | :---: |
      | 0.7564 | 0.8617 | 0.2559 | <br>
    - 若干学習の進みがよくなった。<br>
  - ver32<br>
    - mAPの計算方法から考えると予測個数を増やす分にはスコアは上がっていくと思われるので、make_predictionsの中のthresholdを0.01から1e-7に落とした。<br>
    - | train loss | valid loss | mAP |
      | :---: | :---: | :---: |
      | 0.7566 | 0.8620 | 0.2512 | <br>
    - 変わらないのはありえるにしても落ちるのは考えていなかった。mAPの理解が足りていないのかもしれない。<br>
    - [vinbigdataのchrisさんのディスカッション](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/discussion/229637)を見ても、やはり増やす分には問題がない。ということは、増やしてもほぼ効果がなく、かつスコアのランダム性で若干落ちたのだと考えられる。よく見たらseedの固定が甘かった...<br>
  - ver33<br>
    - 学習率を1e-4に落としたが悪化した。<br>
### 20210716<br>
- アイデアを全然書いてなかったので、githubに色々かいた。<br>
- 002<br>
  - ver34<br>
    - seedを完全に固定して(torch.backends.cudnn.deterministic=True, torch.backends.cudnn.benchmark=False)ver31を回した。<br>
    - | train loss | valid loss | mAP |
      | :---: | :---: | :---: |
      | 0.7561 | 0.8637 | 0.2495 | <br>
    - 若干スコアが落ちているが乱数の影響なので気にしない。<br>
  - ver35<br>
    - seedを完全に固定してver32を回した。<br>
    - | train loss | valid loss | mAP |
      | :---: | :---: | :---: |
      | 0.7561 | 0.8637 | 0.2495 | <br>
    - confidenceが0.01未満のboxは予測に関わってこないらしい。やはりスコアは下がることはないのでこのままにしておく。<br>
  - ver36<br>
    - WBFを実装してみた。<br>
    - | train loss | valid loss | mAP |
      | :---: | :---: | :---: |
      | 0.7561 | 0.8637 | 0.0929 | <br>
    - 予測個数がかなり減ってしまった影響だと考えられる。みてみたがスカスカだった。それでもしっかり当てているので別に実装が間違っているということはなさそう。<br>
    - iou_thresholdはNMSとほぼ同じだからわかるが、skip_box_thresholdの意味がわからない。論文にもロクな説明がなかった。<br>
    - 実装をみたらわかった。この値よりconfが低いboxは削除されるらしい。今回のmAPはboxは増えていいので、この値は0でいいはず。<br>
    - [原論文](https://arxiv.org/pdf/1910.13302.pdf)には、単一のモデルの予測に使うと精度が悪化するとある。TTAをして複数個の予測を作ってからWBFをする必要がある。<br>
  - ver37<br>
    - TTAをする前に、skip_box_thresholdの値を0にしてみた。<br>
- 006<br>
  - 002_ver29-003_ver24-4, 002_ver29-003_ver24-5<br>
    - それぞれopacityとnoneのみでsubした。
    - | class | CV | LB |
      | :---: | :---: | :---: |
      | opacity | 0.0428 | 0.077 |
      | none | 0.1218 | 0.102 | <br>
    - 意外にopacityのスコアが高くてnoneのスコアが低い。noneクラスはもっといけるとは思うが、overfitしてる可能性は高いので、外部データを入れるなりマルチタスク化するなりしないといけない。<br>
  - 002_ver29-003_ver24-6<br>
    - [VinBigDataのGuanshuo Xuさんのディスカッション](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/discussion/229770)で、(noneでない確率) ** nをopacityにかける(Xuさんは0.2にしていた)という処理を行うという発言があったので実際に試してみた。<br>
    - | n | mAP |
      | :---: | :---: |
      | 0.1 | 0.259320 |
      | 0.2 | 0.261252 |
      | 0.3 | 0.261938 |
      | 0.4 | 0.261512 |
    - n=0.3が一番よかったのでそれを採用する。opacityの確率で比重を取ることでmAPをよくするという考え方。もともとしていた、noneの確率が0.8を超えるデータについてはopacityの予測を取り除くという処理を一緒にやってみたらスコアが落ちたのでそれは除外した。<br>
    - | CV | LB |
      | :---: | :---: |
      | 0.1655 | 0.182 |<br>
    - 少しだが確実に上がっているので今後も使い続ける。<br>

### 20210718<br>
- 002<br>
  - ver37<br>
    - skip_box_thresholdを0にした。mAPが0.17~まで上がっていたのでやはり間違いなかった。しかし、予測を見ると大分減っている(大体半分ぐらい)ので、それなりの数のboxが混ぜられたらしい。iou_threshを変えながら様子を見てみた。<br>
    - | iou_thresh | mAP |
      | :---: | :---: |
      | 0.45 | 0.170246 |
      | 0.50 | 0.242080 |
      | 0.55 | 0.253263 |
      | 0.60 | 0.252275 |<br>
    - 原論文通りで、0.55が一番よかった。TTAをした後にどうなるかはわからないが、一旦この値で比較してみる。<br>
  - ver38<br>
    - TTAを実装して、TTAの回数ごとにmAPを計算した。WBFを用いていて、iou_threshdoldは0.6。<br>
    - | TTA | mAP |
      | :---: | :---: |
      | 1 | 0.252275 |
      | 2 | 0.170850 |
      | 3 | 0.186038 |
      | 4 | 0.135670 |
      | 5 | 0.132810 |
      | 6 | 0.114838 |
      | 7 | 0.097058 |
      | 8 | 0.071985 |<br>
    - TTAをやるごとにスコアが下がっているのは、TTAをしていないときに合わせてiou_threshを上げたからだと考えられる。iou_threshをあげると混ぜられずに残るbboxが増えるので、1つのモデルだけならそれでいいが、たくさんの予測がそのまま生き残ってしまうとFPが増えてしまう。<br>
    - よく考えたらimagesを反転させたのにbboxの座標を反転させ直すのを忘れてたので座標が全部反転してしまっていた。そりゃやればやるほど落ちるわ...<br>
    - それとは別で、skip_box_threshを0にしているからか、とても計算が重かった。NMSの時は0でも0.01でも変わりなかったから、0.01にして計算量を減らしても以下もしれない。<br>
    - skip_box_threshは0.01にしてもスコアが変わらなかった。<br>
### 20210720<br>
- 002<br>
  - ver39, ver40<br>
    - iou_threshを変えながらWBFを適用してmAPを計算した。TTAの実装は[このNotebook](https://www.kaggle.com/shonenkov/wbf-over-tta-single-model-efficientdet)を参考にした。<br>
    - | iou_thr\TTA | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
      | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
      | 0.40 | 0.154260 | 0.182200 | 0.194400 | 0.199104 | 0.205522 | 0.209720 | **0.218064** | 0.216368 |
      | 0.45 | 0.170246 | 0.208891 | 0.219482 | 0.223817 | 0.236323 | 0.237921 | 0.239357 | **0.239492** |
      | 0.50 |  0.242080 | 0.237871| 0.239051 | 0.253142 | 0.251299 | 0.254216 | 0.256417 | **0.262196** |
      | 0.55 | 0.253263 | 0.256867 | 0.261684 | 0.266026 | 0.267409 | 0.273478 | **0.276994** | 0.276177 |
      | 0.60 | 0.252275 | 0.264300 | 0.269555 | 0.269084 | 0.273862 | 0.276459 | 0.276928 | **0.278356** |
      | 0.65 | 0.256670 | 0.265052 | 0.271165 | 0.269275 | 0.272705 | 0.275570 | 0.276806 | 0.279867 |
      | 0.70 | 0.259962 | 0.261966 | 0.265956 | 0.264781 | 0.268622 | 0.267630 | 0.270709 | 0.270636 |
      | 0.75 | 0.261023 | 0.251460 | 0.259795 | 0.258474 | 0.262128 | 0.262242 | 0.262361 | 0.262955 |<br>
    - mAPが上がっている。ただ、計算時間が最大で50m~1h程度かかるので、普段はTTAは3回ぐらいで抑えた方がいいかな。<br>

### 20210721<br>
- 002<br>
  - ver41<br>
    - ver39, 40と同じ内容をNMSでやった。<br>
    - | iou_thr\TTA | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
      | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
      | 0.40 | 0.242868 | 0.249600 | 0.251743 | **0.252385** | 0.251778 | 0.251100 | 0.248799 | 0.248799 |
      | 0.45 | 0.249535 | 0.255431 | 0.258217 | **0.259565** | 0.257934 | 0.256658 | 0.255898 | 0.256638 |
      | 0.50 | 0.261517 | 0.262028 | **0.263797** | 0.263485 | 0.260934 | 0.260607 | 0.259960 | 0.260537 |
      | 0.55 | 0.261432 | 0.262817 | **0.264566** | 0.264028 | 0.261487 | 0.261494 | 0.260908 | 0.261351 |
      | 0.60 | 0.261428 | 0.263027 | **0.263555** | 0.262987 | 0.261038 | 0.260353 | 0.259565 | 0.259751 |
      | 0.65 | 0.261493 | **0.262221** | 0.262084 | 0.261133 | 0.258947 | 0.257825 | 0.256852 | 0.257885 |
      | 0.70 | **0.261507** | 0.260746 | 0.258392 | 0.256639 | 0.254099 | 0.252582 | 0.251540 | 0.251624 |
      | 0.75 | **0.261447** | 0.255826 | 0.252386 | 0.250183 | 0.246874 | 0.244414 | 0.242181 | 0.241300 |<br>
    - 計算時間は圧倒的にこっちが早いが、mAPは低い。また、TTAをやればやるほど上がっていくわけでもないので、TTAの結果をうまく削減しながらまとめられるのはWBFだった。<br>
  - ver42<br>
    - submitするときにCVを計算できるように、NMS、iou_thr=0.55, TTA2回の条件で推論のみ行った。モデルはver37のものを使っている。また、pklファイルは重いのでcsvに変えた。<br>
  - ver43<br>
    - ver42と同じ目的で、WBF、iou_thr=0.65、TTA2回で推論のみ行った。モデルも同じ。<br>
  - ver44<br>
    - ver43からTTAを7回に変えた。<br>
- 003<br>
  - ver24から、元画像のサイズを256から512に変えた。<br>
- 004<br>
  - ver13<br>
    - モデルをb6にした。<br>
    - | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: |
      | 0.3473 | 0.3844 | 0.5395 |<br>
    - よくならなかったので、モデルや画像サイズでの改善はもう見込めないとする。[かえるさんのディスカッション](https://www.kaggle.com/c/siim-covid19-detection/discussion/240233)を参考にしてAUX lossを実装してみる。<br>
- 006<br>
  - 002_ver42-003_ver24-1<br>
    - 間違えてTTAをやらないままサブしてしまった。参考記録として残しておく。<br>
    - CV: 0.25ちょい LB:
  - 002_ver42-003_ver24-2<br>
    - TTAをちゃんと2回行った。<br>
    - | opacity mAP | none mAP | CV | LB |
      | :---: | :---: | :---: | :---: |
      | 0.267580 | 0.731090 | 0.1664 | 0.186 |<br>
  - 002_ver43-003_ver24<br>
    - | opacity mAP | none mAP | CV | LB |
      | :---: | :---: | :---: | :---: |
      | 0.276015 | 0.731090 | 0.1679 | 0.187 |<br>
  - 002_ver44-003_ver24<br>
    - | opacity mAP | none mAP | CV | LB |
      | :---: | :---: | :---: | :---: |
      | 0.283403 | 0.731090 | 0.1691 | 0.189 |<br>
    - サブに4時間以上かかったので、ちょっと重たすぎるか。<br>
- 009(2class_step1)<br>
  - RANZCRの3stepのトレーニングを真似して、まずはopacityクラスのbboxを使ってmaskを作って、maskをかけた画像を使って学習をさせてそれを親モデルとし、次に子モデルを普通に学習させつつ親モデルの途中の特徴量マップと子モデルの途中の特徴量マップをAUX Lossを使って近づけ、最後に普通に学習させる3step学習を行った。このNotebookはそれのstep1。<br>
  - ver1<br>
    - 003_ver11からコピーして、かめさんのudemyの講座を参考にしてmaskをつけた。<br>
    - lossがあっという間に収束するので、epochを4に削った。<br>
- 010(2class_step2)<br>
  - ver1<br>
    - 009で作った親モデルの出力の途中の特徴量マップとのAUX Lossをかけつつ子モデルを作った。<br>
    - 親モデルの方の入力にannotationをつけるのを忘れていて失敗した。<br>
  - ver2<br>
    - 親モデルの入力にannotationをつけた。<br>
    - | train loss | valid loss | mAP |
      | :---: | :---: | :---: |
      | 0.7389 | 0.3243 | 0.7301 |<br>
- 011(2class_step3)<br>
  - ver1<br>
    - 010_ver1のモデルを用いて、AUX Lossを外してモデルを作った。<br>
    - | train loss | valid loss | mAP |
      | :---: | :---: | :---: |
      | 0.2106 | 0.3197 | 0.7421 |<br>
    - かなりmAPが改善された。これは期待できそう。<br>

### 20210722<br>
- 006<br>
  - 002_ver43-011_ver1<br>
    - 002_ver44は計算が重たすぎるので、002_ver43をとった。<br>
    - | opacity mAP | none mAP | CV | LB |
      | :---: | :---: | :---: | :---: |
      | 0.276887 | 0.742109 | 0.1698 | - |<br>
    - EfficientNetに使うtimmとEfficientDetに使うtimmのversionがズレているため、どちらかに合わせるとどちらかが動かなくなってしまう。色々試してみたが、途中でtimmのversionを切り替えることができなかったので、EfficientNetの方のversionをなんとか合わせるしかない。<br>
  - 002_ver43-003_ver24-2<br>
    - 2classの方のモデルは、本来画像サイズは384のはずなのに推論時にEfficientDetに合わせて512になってしまっていたので直した。<br>
    - | opacity mAP | none mAP | CV | LB |
      | :---: | :---: | :---: | :---: |
      | 0.276015 | 0.731090 | 0.1679 | 0.194 |<br>
    - もう少し上がってもいいような気はしたが、まあこんなもんか。<br>
  - 002_ver43-003_ver24-3<br>
    - 2classの方にもTTAを実装した。<br>

### 20210723<br>
- 007<br>
  - 004_ver11-2<br>
    - TTAを実装した。<br>
    - | CV | LB |
      | :---: | :---: |
      | 0.5470 | 0.378 |<br>
    - かなり伸びたが、これでも公開Notebookより低いのでなんとかしなくてはいけない。<br>

### 20210727<br>
- ここ数日間はここに記録しないまま進めてしまっていたので、一気に書く。<br>
- 002<br>
  - ver45<br>
    - 011_ver1の重みをEfficientDetのbackboneにロードしてから学習させた。マルチタスク学習をいきなりさせるのは手間なので、一種の近似(？)と考えてもいいこの形で試してみる。<br>
    - | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: |
      | 0.7995 | 0.8795 | 0.2712 |<br>
    - 学習が進むのがかなり早くなったが、その分後半で過学習しただけだった。最適解に近づける効果はないみたい。<br>
- 006<br>
  - 002-ver43-012_ver1<br>
    - | opacity mAP | none mAP | CV | LB |
      | :---: | :---: | :---: | :---: |
      | 0.276887 | 0.742109 | 0.1698 | 0.205 |<br>
    - EfficientDetに合わせたversionのtimmでは、CNNの特徴量が常に(batch_size, -1)に流れてくるせいで、新しいtimmに合わせたモデルではうまくいかなった。途中でいい感じに整形したらうまくいったので安心。落ち着いて考えればすぐにできたことだけど、やたら時間がかかった...<br>
    - AUX Lossが相当効いてる。恐らくnoneクラスに関してはかなりいい線まで来ているのでEfficientDetをなんとかしたい。<br>
    - CVの上がり方とLBの上がり方があまり一致していないのが気になるが、相関はしっかり持っているので頭の片隅に入れる程度に止める。<br>
- 008<br>
  - 002_ver43-011_ver1-014_ver2<br>
    - | opacity | none | negative | typical | indeterminate | atypical | CV | LB |
      | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
      | 0.2769 | 0.7421 | 0.7741| 0.8423 | 0.3046 | 0.3044 | 0.5407 | 0.598 |<br>
  - imageクラスが0.205なのでstudyクラスは0.393。[ディスカッション](https://www.kaggle.com/c/siim-covid19-detection/discussion/248442)をみている限りはまだまだ上がりそう。AUX Lossの入れ方、mモデルの選択あたりは効いていそう。<br>
- 009<br>
  - ver2<br>
    - ver1を全foldで回した。<br>
- 010<br>
  - ver3<br>
    - ver2を全foldで回した。<br>
    - | fold | mAP |
      | :---: | :---: |
      | 0 | 0.7301 |
      | 1 | 0.7261 |
      | 2 | 0.7540 |
      | 3 | 0.7618 |
      | 4 | 0.7637 |<br>
    - 平均的にはmAPは思ったより高かった。最近疎かにしていたが、定期的に全foldで回してちゃんと評価するのは大事。<br>
- 011<br>
  - ver2<br>
    - ver1を全fold回した。<br>
- 012(4class_step1)<br>
  - ver1<br>
    - AUX Lossを入れた3step学習のうちのstep1。<br>
    - | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: |
      | 0.1755 | 0.1704 | 0.7642 |<br>
    - mAPが1に行くかと思ってbest_lossを取る方針にしたが、そうでもなかったのでbest_scoreを取る方針に変える。<br>
  - ver2<br>
    - best_scoreをとった。<br>
    - | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: |
      | 0.1532 | 0.1798 | 0.7794 | <br>
- 013(4class_step2)<br>
  - ver1<br>
    - AUX Lossを入れた3step学習のうちのstep2。<br>
    - | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: |
      | 0.7976 | 0.3849 | 0.5568 |<br>
- 014(4class_step3)<br>
  - ver1<br>
    - AUX Lossを入れた3step学習のうちのstep3。<br>
    - | train_loss | valid_loss | mAP |
      | :---: | :---: | :---: |
      | 0.3165 | 0.3874 | 0.5564 |<br>
    - むしろmAPが悪化してしまった。step2までで止めていいかもしれない。<br>
